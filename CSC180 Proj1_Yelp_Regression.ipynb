{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k47r1\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\k47r1\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdnn/yelp.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# save best model\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32.\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train,y_train,validation_data\u001b[38;5;241m=\u001b[39m(x_test,y_test), batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[monitor,checkpointer],verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m    109\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdnn/yelp.keras\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# load weights from best model\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Predict and measure RMSE\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\k47r1\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\k47r1\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:125\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(x, dtype, sparse)\u001b[0m\n\u001b[0;32m    123\u001b[0m         x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype)\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mSparseTensor):\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "# Author: Nicolas Gugliemo \n",
    "# Date: 9/16/2024\n",
    "# Project: Project 1 Yelp Business Rating Prediction using Tensorflow\n",
    "# Goal: Predict Business's stars rating... \n",
    "# (1) Report the RMSE and plot the lift chart of the BEST neural network model you have obtained.\n",
    "# (2) Choose 5 arbitrary businesses from your test data (preferably from different categories). Show\n",
    "#     the names, the true star ratings, and the predicted ratings (from your best model) of those\n",
    "#     businesses.\n",
    "# Type: Regression (Expect a number)\n",
    "# Data Restrictions:\n",
    "# (1) Businesses with at least 20 reviews\n",
    "# (2) At least 10K businesses in set\n",
    "# (3) Business = busisness_id, stars, review_count, categories\n",
    "# (4) Review   = busisness_id, stars, text \n",
    "'''Grading:  (5 pts) Do train/test split.\n",
    " (5 pts) Remove all the businesses with less than 20 reviews.\n",
    " (10 pts) Use TF-IDF to do feature extraction from review texts.\n",
    " (10 pts) Use EarlyStopping when using Tensorflow.\n",
    " (30 pts) Change the following hyperparameters to record how they affect performance in your report.\n",
    "Tabulate your findings.\n",
    "o Activation: relu, sigmoid, tanh\n",
    "o Layers and neuron counts\n",
    "o Optimizer: adam and sgd\n",
    " (10 pts) Report the RMSE of the BEST regression model you obtained\n",
    " (10 pts) Plot the lift chart on test data of the BEST regression model you obtained\n",
    " (5 pts) Show names and the true ratings of 5 businesses, and their predicted ratings\n",
    " (5 pts) Your report includes the following sections:\n",
    "o Problem Statement\n",
    "o Methodology\n",
    "o Experimental Results and Analysis\n",
    "o Task Division and Project Reflection\n",
    " (10 pts) Additional features\n",
    "'''\n",
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "path = \"./yelp_dataset/\"\n",
    "preprocess = True\n",
    "\n",
    "review_Path = os.path.join(path,\"yelp_academic_dataset_review.json\")\n",
    "business_Path = os.path.join(path,\"yelp_academic_dataset_business.json\")\n",
    "\n",
    "#You may use the following code to convert JSON data into a tabular format Pandas can read.\n",
    "review_df = pd.read_json(review_Path, lines=True, nrows = 10000)\n",
    "all_business_df = pd.read_json(business_Path, lines=True, nrows = 10000)\n",
    "business_df = all_business_df[all_business_df['review_count'] >= 20]\n",
    "business_df = business_df.dropna()\n",
    "review_df = review_df.dropna()\n",
    "\n",
    "#You may use the following code to group ALL the reviews by each business and create a new\n",
    "#dataframe, where each line is a business with all its reviews aggregated together. From there,\n",
    "#you then use tfidfVectorzier to obtain TFIDF representation for each business.\n",
    "df_review_agg = review_df.groupby('business_id')['text'].sum()\n",
    "df_ready_to_be_sent_to_sklearn = pd.DataFrame({'business_id': df_review_agg.index,\n",
    "'all_reviews': df_review_agg.values,})\n",
    "\n",
    "#Create table with ID, stars, and all reviews in one table\n",
    "business_columns = ['business_id', 'stars']\n",
    "review_columns = ['business_id', 'all_reviews']\n",
    "business_subset = business_df[business_columns]\n",
    "review_subset = df_ready_to_be_sent_to_sklearn[review_columns]\n",
    "merged_df = pd.merge(business_subset, df_ready_to_be_sent_to_sklearn, on='business_id', how='inner')\n",
    "#print(merged_df)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the aggregated reviews\n",
    "tfidf_matrix = tfidf.fit_transform(merged_df['all_reviews'])\n",
    "\n",
    "# Convert the result into a DataFrame for better readability \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "#print(tfidf_df)\n",
    "x,y = to_xy(tfidf_df,'stars')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#print(x_train)  # For features\n",
    "#print(y_train)  # For target\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/yelp.keras\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "# batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test), batch_size= 128, callbacks=[monitor,checkpointer],verbose=2,epochs=1000)\n",
    "model.load_weights('dnn/yelp.keras') # load weights from best model\n",
    "\n",
    "# Predict and measure RMSE\n",
    "pred = model.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "# Plot the chart\n",
    "chart_regression(pred.flatten(),y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>all_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tUFrWirKiKi_TAnsVWINQQ</td>\n",
       "      <td>3.5</td>\n",
       "      <td>We are fans of Target.  They seem to have a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MTSW4McQd7CbVtyjqoe9mw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This is nice little Chinese bakery in the hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0bPLkL0QhhPO5kt1_EXmNQ</td>\n",
       "      <td>4.5</td>\n",
       "      <td>The worst Chicken Parm. Sandwich I've ever eat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Stopped in to check out this new spot around t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROeacJQwBeh05Rqg7F6TCg</td>\n",
       "      <td>4.5</td>\n",
       "      <td>This place is fantastic!  Delicious, simple, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>caYHsI0eWE0UyEcSm04uMQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My first time here, after getting a recommenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>SSRNaYae6vFIefAMIbze7w</td>\n",
       "      <td>3.5</td>\n",
       "      <td>I really don't get most of the negative review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>w520vYWEpEYBf90rSr9NVA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>My fiance and I ate here recently, and I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>60NzhNKEQJNBA9ZfvJieDg</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good place for home cooked meal. Great prices....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>Yv6HUVu7fRMnt_NtGdVQBw</td>\n",
       "      <td>3.5</td>\n",
       "      <td>AWFUL. I have been there more than once, and i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1935 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id  stars  \\\n",
       "0     tUFrWirKiKi_TAnsVWINQQ    3.5   \n",
       "1     MTSW4McQd7CbVtyjqoe9mw    4.0   \n",
       "2     0bPLkL0QhhPO5kt1_EXmNQ    4.5   \n",
       "3     MUTTqe8uqyMdBl186RmNeA    4.0   \n",
       "4     ROeacJQwBeh05Rqg7F6TCg    4.5   \n",
       "...                      ...    ...   \n",
       "1930  caYHsI0eWE0UyEcSm04uMQ    5.0   \n",
       "1931  SSRNaYae6vFIefAMIbze7w    3.5   \n",
       "1932  w520vYWEpEYBf90rSr9NVA    4.0   \n",
       "1933  60NzhNKEQJNBA9ZfvJieDg    3.0   \n",
       "1934  Yv6HUVu7fRMnt_NtGdVQBw    3.5   \n",
       "\n",
       "                                            all_reviews  \n",
       "0     We are fans of Target.  They seem to have a li...  \n",
       "1     This is nice little Chinese bakery in the hear...  \n",
       "2     The worst Chicken Parm. Sandwich I've ever eat...  \n",
       "3     Stopped in to check out this new spot around t...  \n",
       "4     This place is fantastic!  Delicious, simple, h...  \n",
       "...                                                 ...  \n",
       "1930  My first time here, after getting a recommenda...  \n",
       "1931  I really don't get most of the negative review...  \n",
       "1932  My fiance and I ate here recently, and I have ...  \n",
       "1933  Good place for home cooked meal. Great prices....  \n",
       "1934  AWFUL. I have been there more than once, and i...  \n",
       "\n",
       "[1935 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "path = \"./yelp_dataset/\"\n",
    "preprocess = True\n",
    "\n",
    "review_Path = os.path.join(path,\"yelp_academic_dataset_review.json\")\n",
    "business_Path = os.path.join(path,\"yelp_academic_dataset_business.json\")\n",
    "\n",
    "#You may use the following code to convert JSON data into a tabular format Pandas can read.\n",
    "review_df = pd.read_json(review_Path, lines=True, nrows = 10000)\n",
    "all_business_df = pd.read_json(business_Path, lines=True, nrows = 10000)\n",
    "business_df = all_business_df[all_business_df['review_count'] >= 20]\n",
    "business_df = business_df.dropna()\n",
    "review_df = review_df.dropna()\n",
    "\n",
    "#You may use the following code to group ALL the reviews by each business and create a new\n",
    "#dataframe, where each line is a business with all its reviews aggregated together. From there,\n",
    "#you then use tfidfVectorzier to obtain TFIDF representation for each business.\n",
    "df_review_agg = review_df.groupby('business_id')['text'].sum()\n",
    "df_ready_to_be_sent_to_sklearn = pd.DataFrame({'business_id': df_review_agg.index,\n",
    "                                               'all_reviews': df_review_agg.values,})\n",
    "\n",
    "#Create table with ID, stars, and all reviews in one table\n",
    "#business_col = ['business_id', 'stars']\n",
    "#review_col = ['business_id', 'all_reviews']\n",
    "business_subset = business_df[['business_id', 'stars']]\n",
    "review_subset = df_ready_to_be_sent_to_sklearn[['business_id', 'all_reviews']]\n",
    "merged_df = pd.merge(business_subset, df_ready_to_be_sent_to_sklearn, on='business_id', how='inner')\n",
    "#print(merged_df)\n",
    "\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the aggregated reviews\n",
    "tfidf_matrix = tfidf.fit_transform(merged_df['all_reviews'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
