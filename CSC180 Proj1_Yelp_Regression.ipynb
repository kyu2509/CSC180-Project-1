{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        print('CLASS')\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        print('REG')\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k47r1\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "136/136 - 18s - 135ms/step - loss: 2.7257 - val_loss: 0.5352\n",
      "Epoch 2/1000\n",
      "136/136 - 2s - 17ms/step - loss: 0.4279 - val_loss: 0.3273\n",
      "Epoch 3/1000\n",
      "136/136 - 2s - 16ms/step - loss: 0.2843 - val_loss: 0.3539\n",
      "Epoch 4/1000\n",
      "136/136 - 1s - 11ms/step - loss: 0.2218 - val_loss: 0.3812\n",
      "Epoch 5/1000\n",
      "136/136 - 1s - 8ms/step - loss: 0.1960 - val_loss: 0.3712\n",
      "Epoch 6/1000\n",
      "136/136 - 1s - 7ms/step - loss: 0.1552 - val_loss: 0.3925\n",
      "Epoch 7/1000\n",
      "136/136 - 1s - 7ms/step - loss: 0.1376 - val_loss: 0.4026\n",
      "Epoch 8/1000\n",
      "136/136 - 1s - 7ms/step - loss: 0.1187 - val_loss: 0.4187\n",
      "Epoch 9/1000\n",
      "136/136 - 1s - 7ms/step - loss: 0.1101 - val_loss: 0.4077\n",
      "Epoch 10/1000\n",
      "136/136 - 1s - 7ms/step - loss: 0.0976 - val_loss: 0.4221\n",
      "Epoch 11/1000\n",
      "136/136 - 1s - 8ms/step - loss: 0.0844 - val_loss: 0.4291\n",
      "Epoch 12/1000\n",
      "136/136 - 1s - 7ms/step - loss: 0.0851 - val_loss: 0.4230\n",
      "Epoch 12: early stopping\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "[[2.9908862]\n",
      " [3.203287 ]\n",
      " [2.532688 ]\n",
      " ...\n",
      " [4.6482296]\n",
      " [3.6707726]\n",
      " [4.0313897]]\n",
      "Score (RMSE): 0.5721346735954285\n",
      "1. Target tUFrWirKiKi_TAnsVWINQQ: rating: 3.5, predicted rating: [2.9908862]\n",
      "2. St Honore Pastries MTSW4McQd7CbVtyjqoe9mw: rating: 4.0, predicted rating: [3.203287]\n",
      "3. Denny's il_Ro8jwPlHresjw9EGmBg: rating: 2.5, predicted rating: [2.532688]\n",
      "4. Zio's Italian Market 0bPLkL0QhhPO5kt1_EXmNQ: rating: 4.5, predicted rating: [3.6173155]\n",
      "5. Tuna Bar MUTTqe8uqyMdBl186RmNeA: rating: 4.0, predicted rating: [3.4470496]\n",
      "6. BAP ROeacJQwBeh05Rqg7F6TCg: rating: 4.5, predicted rating: [2.796009]\n",
      "7. Roast Coffeehouse and Wine Bar WKMJwqnfZKsAae75RMP6jA: rating: 4.0, predicted rating: [3.7323716]\n",
      "8. Barnes & Noble Booksellers qhDdDeI3K4jy2KyzwFN53w: rating: 4.0, predicted rating: [3.854526]\n",
      "9. Romano's Macaroni Grill 9OG5YkX1g2GReZM0AskizA: rating: 2.5, predicted rating: [3.460358]\n",
      "10. H&M noByYNtDLQAra9ccqxdfDw: rating: 3.0, predicted rating: [2.6505263]\n",
      "102. Cafe Con Leche py5aKmlTB2NarfsfcOpHOQ: rating: 2.5, predicted rating: [3.3340871]\n",
      "1001. Gazebo Apartments ZfbeERrUqkP48cr0rey3Mg: rating: 3.0, predicted rating: [3.9805257]\n",
      "  name             business_id  stars_for_model  \\\n",
      "9  H&M  noByYNtDLQAra9ccqxdfDw              3.0   \n",
      "\n",
      "                                         all_reviews  \n",
      "9  The people at the store amazing! We came in to...  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can only merge Series or DataFrame objects, a <class 'numpy.ndarray'> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 136\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sb\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    135\u001b[0m     businesses_s \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(sb, businesses, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 136\u001b[0m     y_s \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(y, businesses_s, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m     b_id_s \u001b[38;5;241m=\u001b[39m businesses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: rating: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, businesses_s[i], b_id_s[i], y_s[i]))\n",
      "File \u001b[1;32mc:\\Users\\k47r1\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:152\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 152\u001b[0m     left_df \u001b[38;5;241m=\u001b[39m _validate_operand(left)\n\u001b[0;32m    153\u001b[0m     right_df \u001b[38;5;241m=\u001b[39m _validate_operand(right)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\k47r1\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2692\u001b[0m, in \u001b[0;36m_validate_operand\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   2691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2693\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only merge Series or DataFrame objects, a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2694\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Can only merge Series or DataFrame objects, a <class 'numpy.ndarray'> was passed"
     ]
    }
   ],
   "source": [
    "# Author: Nicolas Gugliemo, Katrina Yu\n",
    "# Date: 9/16/2024\n",
    "# Project: Project 1 Yelp Business Rating Prediction using Tensorflow\n",
    "# Goal: Predict Business's stars rating... \n",
    "# (1) Report the RMSE and plot the lift chart of the BEST neural network model you have obtained.\n",
    "# (2) Choose 5 arbitrary businesses from your test data (preferably from different categories). Show\n",
    "#     the names, the true star ratings, and the predicted ratings (from your best model) of those\n",
    "#     businesses.\n",
    "# Type: Regression (Expect a number)\n",
    "# Data Restrictions:\n",
    "# (1) Businesses with at least 20 reviews\n",
    "# (2) At least 10K businesses in set\n",
    "# (3) Business = busisness_id, stars, review_count, categories\n",
    "# (4) Review   = busisness_id, stars, text \n",
    "'''Grading:  (5 pts) Do train/test split.\n",
    " (5 pts) Remove all the businesses with less than 20 reviews.\n",
    " (10 pts) Use TF-IDF to do feature extraction from review texts.\n",
    " (10 pts) Use EarlyStopping when using Tensorflow.\n",
    " (30 pts) Change the following hyperparameters to record how they affect performance in your report.\n",
    "Tabulate your findings.\n",
    "o Activation: relu, sigmoid, tanh\n",
    "o Layers and neuron counts\n",
    "o Optimizer: adam and sgd\n",
    " (10 pts) Report the RMSE of the BEST regression model you obtained\n",
    " (10 pts) Plot the lift chart on test data of the BEST regression model you obtained\n",
    " (5 pts) Show names and the true ratings of 5 businesses, and their predicted ratings\n",
    " (5 pts) Your report includes the following sections:\n",
    "o Problem Statement\n",
    "o Methodology\n",
    "o Experimental Results and Analysis\n",
    "o Task Division and Project Reflection\n",
    " (10 pts) Additional features\n",
    "'''\n",
    "import random\n",
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Set path and preprocess for functions\n",
    "path = \"./yelp_dataset/\"\n",
    "preprocess = True\n",
    "\n",
    "#Set paths for JSON\n",
    "review_Path = os.path.join(path,\"yelp_academic_dataset_review.json\")\n",
    "business_Path = os.path.join(path,\"yelp_academic_dataset_business.json\")\n",
    "\n",
    "# Read JSON data and drop businesses with less than 20 reviews\n",
    "review_df = pd.read_json(review_Path, lines=True, nrows = 100000)\n",
    "all_business_df = pd.read_json(business_Path, lines=True, nrows = 100000)\n",
    "business_df = all_business_df[all_business_df['review_count'] >= 20]\n",
    "\n",
    "#You may use the following code to group ALL the reviews by each business and create a new\n",
    "#dataframe, where each line is a business with all its reviews aggregated together. From there,\n",
    "#you then use tfidfVectorzier to obtain TFIDF representation for each business.\n",
    "df_review_agg = review_df.groupby('business_id')['text'].sum()\n",
    "df_ready_to_be_sent_to_sklearn = pd.DataFrame({'business_id': df_review_agg.index,\n",
    "                                               'all_reviews': df_review_agg.values,})\n",
    "\n",
    "#Create table with ID, stars, and all reviews in one table then change 'stars' since this is already a col on tfidf\n",
    "business_subset = business_df[['business_id', 'stars']]\n",
    "business_names = business_df[['name', 'business_id']]\n",
    "review_subset = df_ready_to_be_sent_to_sklearn[['business_id', 'all_reviews']]\n",
    "merged_df = pd.merge(business_subset, review_subset, on='business_id', how='inner')\n",
    "merged_df.rename(columns={'stars': 'stars_for_model'}, inplace=True)\n",
    "\n",
    "# Initialize the tfidf Vectorizer and set max features and stop words\n",
    "tfidf = TfidfVectorizer(max_features=4000,stop_words=\"english\")\n",
    "\n",
    "# Transform all_reviews and turn into a matrix\n",
    "tfidf_matrix = tfidf.fit_transform(merged_df['all_reviews'])\n",
    "\n",
    "# Turn the matrix into a dataframe where the cols are all the words. Then add the stars into the dataframe for model to use and use to_xy for tensorflow to read\n",
    "tfidf_df = pd.DataFrame(data = tfidf_matrix.toarray(), columns = tfidf.get_feature_names_out())\n",
    "tfidf_df = pd.concat([merged_df['stars_for_model'], tfidf_df] , axis=1)\n",
    "x,y = to_xy(tfidf_df,'stars_for_model')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#Run the model 5 times to ensure best model is found\n",
    "i = 0\n",
    "while (i < 1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu')) \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(25, input_dim=x.shape[1], activation='relu')) \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=0.001, beta_1=0.999, beta_2=0.999, epsilon=None, amsgrad=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\n",
    "    checkpointer = ModelCheckpoint(filepath=\"dnn/yelp.keras\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "    # batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "    model.fit(x_train, y_train, validation_data=(x_test,y_test), batch_size= 32, callbacks=[monitor,checkpointer], verbose=2, epochs=1000)\n",
    "    i = i+1\n",
    "\n",
    "model.load_weights('dnn/yelp.keras') # load weights from best model\n",
    "\n",
    "# Predict and measure RMSE\n",
    "pred = model.predict(x_test)\n",
    "print(pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "# Merged df to connect business names back to the star ratings\n",
    "merged_name_star = pd.merge(business_names, merged_df, on='business_id', how='inner')\n",
    "businesses = merged_name_star['name']\n",
    "b_id = merged_name_star['business_id']\n",
    "\n",
    "# Specific data prediction for 10 businesses\n",
    "for j in range(10):\n",
    "    #j = random.randint(0, (pred.size) - 1)\n",
    "    print(\"{}. {} {}: rating: {}, predicted rating: {}\".format(j+1, businesses[j], b_id[j], y[j], pred[j]))\n",
    "\n",
    "print(\"{}. {} {}: rating: {}, predicted rating: {}\".format(101+1, businesses[101], b_id[101], y[101], pred[101]))\n",
    "j=1000\n",
    "print(\"{}. {} {}: rating: {}, predicted rating: {}\".format(j+1, businesses[j], b_id[j], y[j], pred[j]))\n",
    "\n",
    "sb = businesses[businesses['name'] == 'H&M']\n",
    "print(sb)\n",
    "for i in range(sb.size - 1):\n",
    "    businesses_s = pd.merge(sb, businesses, on='name', how='inner')\n",
    "    y_s = pd.merge(y, businesses_s, on='business_id', how=\"inner\")\n",
    "    b_id_s = businesses['business_id']\n",
    "    \n",
    "    print(\"{}. {} {}: rating: {}\".format(i+1, businesses_s[i], b_id_s[i], y_s[i]))\n",
    "\n",
    "# Plot the chart\n",
    "chart_regression(pred.flatten(),y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
